{
  "name": "wsscraper",
  "description": "Easy scraping and auth for JSON/XML web services. Based on node-scraper so it does web page scraping using jQuery too.",
  "version": "0.0.1",
  "author": {
    "name": "Dave Jeffery",
    "email": "dave@davejeffery.com"
  },
  "contributors": [
    {
      "name": "Mathias Pettersson",
      "email": "mape@mape.me"
    }
  ],
  "engines": [
    "node"
  ],
  "directories": {
    "lib": "./lib"
  },
  "main": "./lib/wsscraper",
  "repository": {
    "type": "git",
    "url": "https://github.com/davej/node-wsscraper.git"
  },
  "dependencies": {
    "request": ">=0.10.0",
    "jsdom": ">=0.1.20",
    "xml2js": ">=0.1.5"
  },
  "readme": "# node-wsscraper\n\nA little module that makes scraping and performing auth with JSON/XML web services a little easier. If no JSON/XML web service is available then it can scrape ordinary webpages using JQuery. Uses node.js, xml2js and jQuery.\n\nnode-wsscraper is being developed as a future component of the [OJAX++ project](http://www.ucd.ie/ojax/).\n\nThis is a fork of [node-scraper](http://github.com/mape/node-scraper) by [mape](http://github.com/mape/), if you just want webpage scraping then check out his project instead.\n\n## Installation\n\nVia [npm](http://github.com/isaacs/npm):\n\n    $ npm install wsscraper\n\n## To-do\n\n* Add nice wrapper for doing OAuth.\n* Provide the option of using expat for those who require crazy-fast XML parsing.\n* Add more examples and clean up readme.\n\n## Examples\n\n### Simple JSON\nFirst argument is a url as a string, second is the response format, third is a callback which exposes error information, the JSON object and info about the url.\n\n\tvar scraper = require('wsscraper');\n\n\tscraper('http://search.twitter.com/search.json?q=javascript', 'json', function(err, json_object, urlInfo) {\n\t\tif (err) {throw err;}\n\t    for (var i=0; i < json_object.results.length; i++) {\n\t        console.log(json_object.results[i].text+'\\n');\n\t    };\n\t});\n\t\n### Simple XML\nJSON is the preferred response format. When JSON isn't available wsscraper can also parse XML responses and convert them to JSON objects using [xml2js](https://github.com/maqr/node-xml2js). Simply specify 'xml' as the expected response format.\n\n\tvar scraper = require('wsscraper');\n\n\tscraper('http://search.twitter.com/search.atom?q=javascript', 'xml', function(err, json_object, urlInfo) {\n\t\tif (err) {throw err;}\n\t    for (var i=0; i < json_object.entry.length; i++) {\n\t        console.log(json_object.entry[i].title+'\\n')\n\t    };\n\t});\n\n### Simple HTML\nAnd if no web service API is available then we can use JQuery to scrape the webpage. Simply specify 'html' as the expected response format. *Note: This uses jsdom and JQuery so it's pretty slow*.\n\n\tvar scraper = require('wsscraper');\n\n\tscraper('http://search.twitter.com/search?q=javascript', function(err, $, urlInfo) {\n\t\tif (err) {throw err;}\n\t\t$('.msg').each(function() {\n\t\t\tconsole.log($(this).text().trim()+'\\n');\n\t\t});\n\t});\n### \"Advanced\"\nFirst argument is an object containing settings for the \"request\" instance used internally, second is a callback which exposes a jQuery object with your scraped site as \"body\" and third is an object from the request containing info about the url.\n\n    var scraper = require('scraper');\n    scraper(\n\t    {\n           'uri': 'http://search.twitter.com/search?q=nodejs'\n               , 'headers': {\n                   'User-Agent': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)'\n               }\n        }\n\t\t, 'html'\n        , function(err, $) {\n            if (err) {throw err}\n\n            $('.msg').each(function() {\n                console.log($(this).text().trim()+'\\n');\n            });\n        }\n    );\n### Parallel\nFirst argument is an array containing either strings or objects, second is a callback which exposes a jQuery object with your scraped site as \"body\" and third is an object from the request containing info about the url.\n\n**You can also add rate limiting to the fetcher by adding an options object as the third argument containing 'reqPerSec': float.**\n\n    var scraper = require('scraper');\n    scraper(\n\t    [\n            'http://search.twitter.com/search?q=javascript'\n            , 'http://search.twitter.com/search?q=css'\n            , {\n                'uri': 'http://search.twitter.com/search?q=nodejs'\n                , 'headers': {\n                    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)'\n                }\n            }\n            , 'http://search.twitter.com/search?q=html5'\n        ]\n\t\t, 'html'\n        , function(err, $, urlInfo) {\n            if (err) {throw err;}\n\n            console.log('Messages from: '+urlInfo.href);\n            $('.msg').each(function() {\n                console.log($(this).text().trim()+'\\n');\n            });\n        }\n        , {\n            'reqPerSec': 0.2 // Wait 5sec between each external request\n        }\n    );\n\n\n\n## Arguments\n\n### First (required)\nContains the info about what page/pages will be scraped\n\n#### string\n    'http://www.nodejs.org'\n**or**\n\n#### request object\n    {\n       'uri': 'http://search.twitter.com/search?q=nodejs'\n           , 'headers': {\n               'User-Agent': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)'\n           }\n    }\n**or**\n\n#### Array (if you want to do fetches on multiple URLs)\n    [\n        urlString\n        , urlString\n        , requestObject\n        , urlString\n    ]\n\n### Second (optional)\nThe expected response format, defaults to 'html' if not specified.\n\n\t'json'\n**or**\n\t'xml' // Runs XML through xml2js and passes a JSON object to the callback\n**or**\n\t'html' // Creates a DOM for HTML page and passes a JQuerified body to the callback\n\n\n### Third (optional)\nThe callback that allows you do use the data retrieved from the fetch.\n\n    function(err, $, urlInfo) {\n        if (err) {throw err;}\n        \n        /* Showing the data within urlInfo: \n        { href: 'http://search.twitter.com/search?q=javascript',\n          protocol: 'http:',\n          slashes: true,\n          host: 'search.twitter.com',\n          hostname: 'search.twitter.com',\n          search: '?q=javascript',\n          query: 'q=javascript',\n          pathname: '/search',\n          port: 80 }\n        */\n    \n        console.log('Messages from: '+urlInfo.href);\n        $('.msg').each(function() {\n            console.log($(this).text().trim()+'\\n');\n        }\n    }\n\n### Fourth (optional)\nThis argument is an object containing settings for the fetcher overall.\n\n* **reqPerSec**: float; (allows you to throttle your fetches so you don't hammer the server you are scraping)\n\n## Depends on\n* [tmpvar](https://github.com/tmpvar/)'s [jsdom](https://github.com/tmpvar/jsdom)\n* [mikeal](https://github.com/mikeal/)'s [request](https://github.com/mikeal/node-utils/tree/master/request)\n[maqr](https://github.com/maqr/)'s [xml2js](https://github.com/maqr/node-xml2js)\n* [jquery](https://github.com/jquery/jquery)",
  "readmeFilename": "README.md",
  "_id": "wsscraper@0.0.1",
  "dist": {
    "shasum": "fd5c1458fa1a93c1b3f44177be8108a07452cb18"
  },
  "_from": "wsscraper"
}
